# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

spark.master                     yarn
spark.speculation                true
spark.speculation.multiplier     2
spark.local.dir                  /home/vipshop/hard_disk/1/spark
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer      10m
spark.kryoserializer.buffer.max  1024m
spark.file.transferTo            false
spark.scheduler.mode             FAIR
spark.scheduler.listenerbus.eventqueue.capacity    100000

spark.driver.memory              15g
spark.driver.extraClassPath      /home/vipshop/platform/hive/lib/mysql-connector-java-5.1.25.jar:/home/vipshop/platform/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/vipshop/bi/spark_auxlib/hiveserver-auth-pass.jar:/home/vipshop/bi/spark_auxlib/spark-plugins-0.1.0-SNAPSHOT-jar-with-dependencies.jar

# add for the problem about native Snappy 2017-6-6 davidlin roncenzhao
spark.executor.extraLibraryPath  /home/vipshop/platform/hadoop/lib/native
spark.executor.extraClassPath    /home/vipshop/bi/libutil/com.google.guava_guava-16.0.1.jar:/home/vipshop/platform/hive/lib/mysql-connector-java-5.1.25.jar:/home/vipshop/platform/hadoop/share/hadoop/common/lib/gson-2.2.4.jar

spark.executor.extraJavaOptions -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Djava.net.preferIPv4Stack=true -Xloggc:<LOG_DIR>/executor_gc.out -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=<LOG_DIR>/executor_dump.out -XX:OnOutOfMemoryError='kill -9 %p' -Dalluxio.zookeeper.address=sd-hadoop-biphotdata-jn1.idc.vip.com:2181,sd-hadoop-biphotdata-jn2.idc.vip.com:2181,sd-hadoop-biphotdata-jn3.idc.vip.com:2181,sd-hadoop-biphotdata-nn1.idc.vip.com:2181,sd-hadoop-biphotdata-nn2.idc.vip.com:2181 -Dalluxio.zookeeper.leader.path=/alluxio_biphotdata/leader -Dalluxio.zookeeper.enabled=true -XX:+PrintGCApplicationStoppedTime

spark.executor.memory                  5g
spark.executor.cores                   4
spark.executor.memoryOverhead          3g
spark.executor.heartbeatInterval       20s

spark.yarn.am.memory    1500m
spark.yarn.am.cores     1
spark.yarn.am.extraJavaOptions -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Djava.net.preferIPv4Stack=true -Xloggc:<LOG_DIR>/executor_gc.out -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=<LOG_DIR>/executor_dump.out -XX:OnOutOfMemoryError='kill -9 %p'

spark.driver.extraJavaOptions -Dalluxio.zookeeper.address=sd-hadoop-biphotdata-jn1.idc.vip.com:2181,sd-hadoop-biphotdata-jn2.idc.vip.com:2181,sd-hadoop-biphotdata-jn3.idc.vip.com:2181,sd-hadoop-biphotdata-nn1.idc.vip.com:2181,sd-hadoop-biphotdata-nn2.idc.vip.com:2181 -Dalluxio.zookeeper.leader.path=/alluxio_biphotdata/leader -Dalluxio.zookeeper.enabled=true

spark.yarn.archive      hdfs://bipcluster/dp/spark/spark-lib-3.0.1-ec.jar

spark.shuffle.service.enabled   true
spark.shuffle.consolidateFiles  true

spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors    1
spark.dynamicAllocation.maxExecutors    400
spark.dynamicAllocation.executorIdleTimeout     60s
spark.dynamicAllocation.initialExecutors        1

spark.streaming.concurrentJobs    5

spark.ui.retainedJobs   500
spark.ui.retainedStages 500
spark.ui.port   0
spark.ui.killEnabled    true

spark.eventLog.enabled  true
spark.eventLog.compress false
spark.eventLog.dir      hdfs://bipcluster03/spark/spark-jobhistory
spark.history.fs.logDirectory hdfs://bipcluster03/spark/spark-jobhistory
spark.history.fs.cleaner.enabled true
spark.history.fs.cleaner.maxAge 7d

#Dynamic memory management
spark.memory.useLegacyMode      false
spark.memory.Fraction   0.7

spark.memory.storageFraction    0.5
spark.memory.offHeap.size       6g

spark.yarn.historyServer.address        10.208.54.91:18080
spark.yarn.report.interval   6s

spark.locality.wait.process    10ms
spark.locality.wait.node       20ms
spark.yarn.maxAppAttempts      0

spark.client.extraClassPath    /home/vipshop/bi/spark_auxlib/spark-plugins-0.1.0-SNAPSHOT-jar-with-dependencies.jar
#spark.app.submit.pre.hook    com.vip.dp.spark.app.controller.SparkPreAppSubmitHookImpl
#spark.app.submit.pre.hook null

#spark.sql.autoBroadcastJoinThreshold 31457280
spark.sql.shuffle.partitions           99
spark.sql.statistics.fallBackToHdfs    true
spark.sql.statistics.partitionPruner   true
#spark.sql.adaptive.enabled             true
spark.broadcast.blockSize              16m
spark.sql.orc.enabled                  true
spark.sql.orc.filterPushdown           true
spark.sql.hive.convertMetastoreOrc     true
#spark.sql.orc.impl native
spark.sql.objectHashAggregate.sortBased.fallbackThreshold   512
spark.sql.files.ignoreCorruptFiles     false
spark.sql.filesourceTableRelationCacheSize 0
spark.sql.hive.caseSensitiveInferenceMode NEVER_INFER
spark.sql.crossJoin.enabled            true
# merge false
spark.sql.global.merge.output.enabled	false

spark.rpc.lookupTimeout      120s
spark.rpc.askTimeout         120s
spark.network.timeout        600s
spark.port.maxRetries        100
spark.rpc.message.maxSize    256



# spark shuffle service tuning

spark.shuffle.io.serverThreads  128
spark.shuffle.io.backLog  8192
spark.shuffle.io.numConnectionsPerPeer 4
spark.shuffle.service.index.cache.size    2048
spark.shuffle.io.maxRetries 8
spark.io.compression.lz4.blockSize  512k
spark.rpc.io.serverThreads  64
spark.memory.offHeap.enabled      true
spark.shuffle.file.buffer  1m
spark.unsafe.sorter.spill.reader.buffer.size 1m
spark.reducer.maxBlocksInFlightPerAddress 512

spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs   true
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version    2
spark.hadoop.dfs.client.hedged.read.threadpool.size             0
spark.hadoop.dfs.client.hedged.read.threshold.millis            10000

spark.hadoop.dfs.client.socket-timeout 5000

spark.hadoop.dfs.client.ec.fast.switch.read.enabled             false
spark.hadoop.dfs.client.slow.packet.threshold.ms                10

spark.hadoop.dfs.client.replica.fast.switch.read.enabled false
spark.hadoop.dfs.client.ec.fast.switch.read.enabled false
spark.hadoop.dfs.client.slow.packet.threshold.ms    300
spark.hadoop.dfs.client.slow.block.threshold.ms     600000
spark.hadoop.dfs.client.slow.datanode.threshold.ms  1800000

spark.hadoop.mapreduce.input.fileinputformat.split.minsize    256000000
spark.hadoop.mapreduce.input.fileinputformat.split.maxsize    256000000

#spark.driver.userClassPathFirst  true

spark.sql.hive.metastore.sharedPrefixes  com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc,alluxio

spark.sql.extensions  org.apache.spark.sql.hudi.HoodieSparkSessionExtension
# spark.sql.extensions  io.delta.sql.DeltaSparkSessionExtension,org.apache.spark.sql.hudi.HoodieSparkSessionExtension
# spark.sql.catalog.spark_catalog   org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.sql.storeAssignmentPolicy   LEGACY
spark.sql.legacy.timeParserPolicy  LEGACY

spark.sql.shuffledJoin.childrenPartitioningDetection  false
spark.sql.bucketing.coalesceBucketsInJoin.enabled     true

spark.plugins   org.apache.spark.api.plugin.ThreadDumpPlugin,org.apache.spark.api.plugin.ProfilerSparkPlugin
spark.plugin.threadDump.rate.ms           120000
spark.plugin.threadDump.delay.ms          5000
spark.plugin.profiler.interval.seconds    120
spark.plugin.profiler.delay.seconds        5